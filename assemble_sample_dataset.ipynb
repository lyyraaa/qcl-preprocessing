{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-04T10:31:21.665860200Z",
     "start_time": "2025-07-04T10:31:21.655381500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import openpyxl\n",
    "import h5py\n",
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sys\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Ellipsis, slice(None, None, None))\n"
     ]
    }
   ],
   "source": [
    "is_local = True # todo\n",
    "\n",
    "# Experiment\n",
    "seed = 1000 if is_local else int(sys.argv[-2])\n",
    "torch.manual_seed(seed)\n",
    "image_size = 360\n",
    "\n",
    "# Data: which wavenumbers are even allowed to be considered?\n",
    "wv_start = 0\n",
    "wv_end = 340\n",
    "\n",
    "# Data loading\n",
    "batch_size= 64\n",
    "patch_dim = 1\n",
    "samples_to_train = 100000 # todo 10000\n",
    "\n",
    "channels_used = np.s_[...,:] # used only when r_method = 'fixed'\n",
    "print(channels_used)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-07-04T10:31:21.906586200Z",
     "start_time": "2025-07-04T10:31:21.896258200Z"
    }
   },
   "id": "8d72b4d3c9acb14c",
   "execution_count": 30
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 166 cores\n",
      "Using 340/340 wavenumbers\n"
     ]
    }
   ],
   "source": [
    "def csf_fp(filepath):\n",
    "    return filepath.replace('D:/datasets','D:/datasets' if is_local else './')\n",
    "\n",
    "master = pd.read_excel(csf_fp(rf'D:/datasets/pcuk2023_qcl2025_whole_core_raw/master_sheet.xlsx'))\n",
    "slide = master['slide'].to_numpy()\n",
    "patient_id = master['patient_id'].to_numpy()\n",
    "hdf5_filepaths = np.array([csf_fp(fp) for fp in master['hdf5_filepath']])\n",
    "annotation_filepaths = np.array([csf_fp(fp) for fp in master['annotation_filepath']])\n",
    "mask_filepaths = np.array([csf_fp(fp) for fp in master['mask_filepath']])\n",
    "wavenumbers = np.load(csf_fp(f'D:/datasets/pcuk2023_ftir_whole_core/wavenumbers.npy'))[wv_start:wv_end]\n",
    "wavenumbers_used = wavenumbers[channels_used]\n",
    "\n",
    "annotation_class_colors = np.array([[0,255,0],[128,0,128],[255,0,255],[0,0,255],[255,165,0],[255,0,0]])\n",
    "annotation_class_names = np.array(['epithelium_n','stroma_n','epithelium_c','stroma_c','corpora_amylacea','blood'])\n",
    "n_classes = len(annotation_class_names)\n",
    "print(f\"Loaded {len(slide)} cores\")\n",
    "print(f\"Using {len(wavenumbers_used)}/{len(wavenumbers)} wavenumbers\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-07-04T10:31:22.309378600Z",
     "start_time": "2025-07-04T10:31:22.260049Z"
    }
   },
   "id": "248ad5a009d925b7",
   "execution_count": 31
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class ftir_patching_dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self,hdf5_filepaths, patient_ids,mask_filepaths, annotation_filepaths, channels_use,\n",
    "                 patch_dim=25, augment=True,):\n",
    "        \n",
    "        # Define data paths\n",
    "        self.hdf5_filepaths = hdf5_filepaths\n",
    "        self.patient_ids = patient_ids\n",
    "        self.mask_filepaths = mask_filepaths\n",
    "        self.annotation_filepaths = annotation_filepaths\n",
    "        self.augment = augment\n",
    "        \n",
    "        # patch dimensions\n",
    "        self.patch_dim = patch_dim\n",
    "        self.patch_minus = patch_dim //2; self.patch_plus = 1 + (patch_dim // 2)\n",
    "        self.channels = channels_use\n",
    "        \n",
    "        # class data\n",
    "        self.annotation_class_colors = annotation_class_colors\n",
    "        self.annotation_class_names = annotation_class_names\n",
    "        self.total_sampled = torch.zeros(len(self.annotation_class_colors))\n",
    "        \n",
    "        # Open every core hdf5 file\n",
    "        self.open()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.total_pixels\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        # get patch data\n",
    "        row = self.rows[idx]\n",
    "        col = self.cols[idx]\n",
    "        cidx = self.cidxs[idx]\n",
    "        pidx = self.pidxs[idx]\n",
    "        label = self.tissue_classes[idx]\n",
    "        self.total_sampled[label] += 1\n",
    "        \n",
    "        # Are dimensions of patch okay\n",
    "        idx_u = row - self.patch_minus\n",
    "        idx_d = row + self.patch_plus\n",
    "        idx_l = col - self.patch_minus\n",
    "        idx_r = col + self.patch_plus\n",
    "        pad_u = max(-idx_u,0); idx_u = max(idx_u,0)\n",
    "        pad_d = max(idx_d-image_size,0); idx_d = min(idx_d,image_size)\n",
    "        pad_l = max(-idx_l,0); idx_l = max(idx_l,0)\n",
    "        pad_r = max(idx_r-image_size,0); idx_r = min(idx_r,image_size)\n",
    "        \n",
    "        # get patch\n",
    "        patch = torch.from_numpy(\n",
    "            self.hdf5_files[cidx]['spectra'][idx_u:idx_d,idx_l:idx_r,*self.channels],\n",
    "        ).permute(2,0,1)\n",
    "        patch *= torch.from_numpy(\n",
    "            self.hdf5_files[cidx]['mask'][idx_u:idx_d,idx_l:idx_r,],\n",
    "        ).unsqueeze(0)\n",
    "        \n",
    "        # pad patch\n",
    "        patch = torch.nn.functional.pad(patch,(pad_l,pad_r,pad_u,pad_d,0,0))\n",
    "        \n",
    "        if self.augment:\n",
    "            patch = self.transforms(patch)\n",
    "        return patch,label,pidx\n",
    "\n",
    "    # split annotations from H x W x 3 to C x H x W, one/zerohot along C dimension\n",
    "    def split_annotations(self,annotations_img):\n",
    "        split = torch.zeros((len(self.annotation_class_colors),*annotations_img.shape[:-1]))\n",
    "        for c,col in enumerate(annotation_class_colors):\n",
    "            split[c,:,:] = torch.from_numpy(np.all(annotations_img == self.annotation_class_colors[c],axis=-1)) \n",
    "        return split\n",
    "    \n",
    "    # open every file \n",
    "    def open(self):\n",
    "        self.hdf5_files = []\n",
    "        self.tissue_classes = []\n",
    "        self.rows = []\n",
    "        self.cols = []\n",
    "        self.cidxs = []\n",
    "        self.pidxs = []\n",
    "        \n",
    "        # for every core in dataset,\n",
    "        for cidx in range(0,len(self.hdf5_filepaths)):\n",
    "            # open annotations and remove edges and non-tissue px\n",
    "            annotation = self.split_annotations(cv2.imread(self.annotation_filepaths[cidx])[:,:,::-1])\n",
    "            mask = torch.from_numpy(cv2.imread(self.mask_filepaths[cidx])[:,:,1]) / 255\n",
    "            annotation *= mask\n",
    "            pidx = self.patient_ids[cidx]\n",
    "            # for every class,\n",
    "            for cls in range(len(annotation_class_names)):\n",
    "                # get location of annotations, append to lists\n",
    "                r,c = torch.where(annotation[cls])\n",
    "                num_cls = annotation[cls].sum().int().item()\n",
    "                self.tissue_classes.extend([cls,]*num_cls)\n",
    "                self.cidxs.extend([cidx,]*num_cls)\n",
    "                self.pidxs.extend([pidx,]*num_cls)\n",
    "                self.rows.extend(r)\n",
    "                self.cols.extend(c)\n",
    "            # add open hdf5 file to list\n",
    "            self.hdf5_files.append(h5py.File(self.hdf5_filepaths[cidx],'r'))\n",
    "                \n",
    "        # construct data tensors\n",
    "        self.rows = torch.Tensor(self.rows).int()\n",
    "        self.cols = torch.Tensor(self.cols).int()\n",
    "        self.tissue_classes = torch.Tensor(self.tissue_classes).long()\n",
    "        self.cidxs = torch.Tensor(self.cidxs).int()\n",
    "        self.pidxs = torch.Tensor(self.pidxs).int()\n",
    "        self.total_pixels = len(self.cidxs)\n",
    "\n",
    "    # close every open hdf5 file\n",
    "    def close(self):\n",
    "        for cidx in range(len(self.hdf5_files)):\n",
    "            self.hdf5_files[cidx].close()\n",
    "        self.hdf5_files = []\n",
    "        self.tissue_classes = []\n",
    "        self.xs = []\n",
    "        self.ys = []"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-07-04T10:33:18.488338200Z",
     "start_time": "2025-07-04T10:33:18.486281700Z"
    }
   },
   "id": "f090239aaf8baf33",
   "execution_count": 36
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loader sizes:\n",
      "\tnormal unbalanced: 16816\n",
      "\teven: 16816\n"
     ]
    }
   ],
   "source": [
    "dataset = ftir_patching_dataset(\n",
    "    hdf5_filepaths, patient_id, mask_filepaths, annotation_filepaths, channels_used,\n",
    "    patch_dim = patch_dim, augment=False,\n",
    ")\n",
    "\n",
    "# Instiantiate data loaders\n",
    "_, class_counts = np.unique(dataset.tissue_classes, return_counts=True)\n",
    "class_weights = 1 / class_counts\n",
    "class_weights = class_weights[dataset.tissue_classes]\n",
    "sampler = torch.utils.data.WeightedRandomSampler(class_weights, len(class_weights), replacement=True)\n",
    "\n",
    "loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True,drop_last=True)\n",
    "loader_even = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=sampler,drop_last=True)\n",
    "print(f\"loader sizes:\\n\\tnormal unbalanced: {len(loader)}\\n\\teven: {len(loader_even)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-07-04T10:33:25.632114900Z",
     "start_time": "2025-07-04T10:33:18.943321500Z"
    }
   },
   "id": "c98bdd2301248742",
   "execution_count": 37
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assembled data with 100096 samples and 100096 labels and 100096 pids.\n"
     ]
    }
   ],
   "source": [
    "data_odd = []\n",
    "labels_odd = []\n",
    "pids_odd = []\n",
    "for iter in range(0, (samples_to_train // batch_size) + 1):\n",
    "    for bidx, (data, label, pid) in enumerate(loader):\n",
    "        print(f\"{bidx}/{(samples_to_train // batch_size)}\",end=\"\\r\")\n",
    "        data_odd.append(data.squeeze().cpu().numpy())\n",
    "        labels_odd.append(label.squeeze().cpu().numpy())\n",
    "        pids_odd.append(pid.squeeze().cpu().numpy())\n",
    "        \n",
    "        if iter*len(loader) + bidx*batch_size > samples_to_train:\n",
    "            end = True\n",
    "            break\n",
    "    if end:\n",
    "        break\n",
    "data_odd = np.concatenate(data_odd,axis=0)\n",
    "labels_odd = np.concatenate(labels_odd,axis=0)\n",
    "pids_odd = np.concatenate(pids_odd,axis=0)\n",
    "print(f\"Assembled data with {len(data_odd)} samples and {len(labels_odd)} labels and {len(pids_odd)} pids.\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-07-04T10:34:49.788846500Z",
     "start_time": "2025-07-04T10:33:31.064364300Z"
    }
   },
   "id": "6e678f6ea00e027a",
   "execution_count": 38
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assembled data with 100096 samples and 100096 labels and 100096 pids.\n"
     ]
    }
   ],
   "source": [
    "data_even = []\n",
    "labels_even = []\n",
    "pids_even = []\n",
    "for iter in range(0, (samples_to_train // batch_size) + 1):\n",
    "    for bidx, (data, label, pid) in enumerate(loader_even):\n",
    "        print(f\"{bidx}/{(samples_to_train // batch_size)}\",end=\"\\r\")\n",
    "        data_even.append(data.squeeze().cpu().numpy())\n",
    "        labels_even.append(label.squeeze().cpu().numpy())\n",
    "        pids_even.append(pid.squeeze().cpu().numpy())\n",
    "        \n",
    "        if iter*len(loader_even) + bidx*batch_size > samples_to_train:\n",
    "            end = True\n",
    "            break\n",
    "    if end:\n",
    "        break\n",
    "data_even = np.concatenate(data_even,axis=0)\n",
    "labels_even = np.concatenate(labels_even,axis=0)\n",
    "pids_even = np.concatenate(pids_even,axis=0)\n",
    "print(f\"Assembled data with {len(data_even)} samples and {len(labels_even)} labels and {len(pids_even)} pids.\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-07-04T10:37:29.802374500Z",
     "start_time": "2025-07-04T10:36:44.328091900Z"
    }
   },
   "id": "9b665758657bf344",
   "execution_count": 40
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "np.save(\"./data/data_even.npy\", data_even)\n",
    "np.save(\"./data/labels_even.npy\", labels_even)\n",
    "np.save(\"./data/pids_even.npy\", pids_even)\n",
    "np.save(\"./data/data_odd.npy\", data_odd)\n",
    "np.save(\"./data/labels_odd.npy\", labels_odd)\n",
    "np.save(\"./data/pids_odd.npy\", pids_odd)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-07-04T10:37:39.967762200Z",
     "start_time": "2025-07-04T10:37:39.763784500Z"
    }
   },
   "id": "69693785ad8a2dc4",
   "execution_count": 41
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
